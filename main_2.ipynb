{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f698a841",
   "metadata": {},
   "source": [
    "# AFIB Detection Using 1D CNN on Cleaned ECG Dataset\n",
    "\n",
    "This notebook trains a binary AFIB detector using the cleaned and balanced dataset generated by data_handling.ipynb.\n",
    "\n",
    "Workflow:\n",
    "1. Setup paths and load cleaned mapping\n",
    "2. Verify data and prepare labels\n",
    "3. Split dataset (train/val/test)\n",
    "4. Compute global channel statistics\n",
    "5. Define ECG dataset class\n",
    "6. Create data loaders\n",
    "7. Define and train 1D CNN model\n",
    "8. Evaluate on test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f00e5f",
   "metadata": {},
   "source": [
    "## 1. Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d956d67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"✓ All imports successful\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118822cc",
   "metadata": {},
   "source": [
    "## 2. Configuration & Path Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bc81dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "DOWNSAMPLE = 2  # Downsample factor (e.g., 2 = keep every 2nd sample)\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 30\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paths (all relative to project root)\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "CLEANED_ROOT = DATA_DIR / \"cleaned_balanced_AFIB_SR\"\n",
    "CLEANED_WFDB_DIR = CLEANED_ROOT / \"WFDBRecords\"\n",
    "MAPPING_CSV = CLEANED_ROOT / \"file_mapping_cleaned.csv\"\n",
    "\n",
    "# Model save path\n",
    "MODEL_DIR = PROJECT_ROOT / \"models\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_PATH = MODEL_DIR / \"ecg_1dcnn_afib_balanced.pth\"\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"MAPPING_CSV: {MAPPING_CSV}\")\n",
    "print(f\"MODEL_PATH: {MODEL_PATH}\")\n",
    "print(\"\\n✓ Configuration complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feced3c",
   "metadata": {},
   "source": [
    "## 3. Load Cleaned Mapping & Verify Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed79a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned mapping\n",
    "cleaned_mapping = pd.read_csv(MAPPING_CSV)\n",
    "\n",
    "print(f\"Loaded {len(cleaned_mapping):,} records from {MAPPING_CSV.name}\")\n",
    "print(f\"\\nColumns: {list(cleaned_mapping.columns)}\")\n",
    "\n",
    "# Verify required columns exist\n",
    "required_cols = ['record_id', 'ecg_path', '_AFIB', '_SR']\n",
    "missing_cols = [col for col in required_cols if col not in cleaned_mapping.columns]\n",
    "assert not missing_cols, f\"Missing required columns: {missing_cols}\"\n",
    "print(f\"✓ All required columns present: {required_cols}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\n--- Dataset Statistics ---\")\n",
    "print(f\"Total records: {len(cleaned_mapping):,}\")\n",
    "print(f\"AFIB records (_AFIB=1): {cleaned_mapping['_AFIB'].sum():,}\")\n",
    "print(f\"Non-AFIB records (_AFIB=0): {(cleaned_mapping['_AFIB'] == 0).sum():,}\")\n",
    "print(f\"SR records (_SR=1): {cleaned_mapping['_SR'].sum():,}\")\n",
    "print(f\"Non-SR records (_SR=0): {(cleaned_mapping['_SR'] == 0).sum():,}\")\n",
    "\n",
    "# Display sample rows\n",
    "print(\"\\nSample rows:\")\n",
    "cleaned_mapping.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317f7a44",
   "metadata": {},
   "source": [
    "## 4. Prepare Labels for Binary AFIB Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddadc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use _AFIB as the primary target column\n",
    "target_col = \"_AFIB\"\n",
    "\n",
    "# Ensure target is strictly 0/1\n",
    "cleaned_mapping[target_col] = pd.to_numeric(cleaned_mapping[target_col], errors='coerce').fillna(0)\n",
    "cleaned_mapping[target_col] = (cleaned_mapping[target_col] > 0).astype(int)\n",
    "\n",
    "print(f\"Target column: {target_col}\")\n",
    "print(\"Target distribution:\")\n",
    "print(cleaned_mapping[target_col].value_counts().sort_index())\n",
    "print(\"\\n✓ Labels prepared for binary AFIB detection\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e02ea56",
   "metadata": {},
   "source": [
    "## 5. Split Dataset (Train/Val/Test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4851d249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create indices array\n",
    "indices = np.arange(len(cleaned_mapping))\n",
    "labels = cleaned_mapping[target_col].values\n",
    "\n",
    "# First split: 80% train, 20% temp\n",
    "idx_train, idx_temp, y_train, y_temp = train_test_split(\n",
    "    indices, labels,\n",
    "    test_size=0.2,\n",
    "    stratify=labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Second split: temp into 50% val, 50% test\n",
    "idx_val, idx_test, y_val, y_test = train_test_split(\n",
    "    idx_temp, y_temp,\n",
    "    test_size=0.5,\n",
    "    stratify=y_temp,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"--- Dataset Splits ---\")\n",
    "print(f\"Train: {len(idx_train):,} samples (AFIB: {y_train.sum():,}, Non-AFIB: {(y_train == 0).sum():,})\")\n",
    "print(f\"Val:   {len(idx_val):,} samples (AFIB: {y_val.sum():,}, Non-AFIB: {(y_val == 0).sum():,})\")\n",
    "print(f\"Test:  {len(idx_test):,} samples (AFIB: {y_test.sum():,}, Non-AFIB: {(y_test == 0).sum():,})\")\n",
    "print(\"\\n✓ Dataset split complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662a78d8",
   "metadata": {},
   "source": [
    "## 6. Compute Global Channel Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1f5c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_channel_stats(mapping, indices, downsample=None):\n",
    "    \"\"\"\n",
    "    Compute global per-channel mean and std from a subset of training data.\n",
    "    Stacks up to 512 random samples to estimate statistics.\n",
    "    \"\"\"\n",
    "    if len(indices) > 512:\n",
    "        sample_indices = np.random.choice(indices, size=512, replace=False)\n",
    "    else:\n",
    "        sample_indices = indices\n",
    "\n",
    "    data_list = []\n",
    "\n",
    "    for idx in sample_indices:\n",
    "        row = mapping.iloc[int(idx)]\n",
    "        npy_file = Path(row[\"ecg_path\"])\n",
    "        if not npy_file.exists():\n",
    "            continue\n",
    "        try:\n",
    "            data = np.load(npy_file)\n",
    "            # Ensure 2D array and shape (12, L)\n",
    "            if data.ndim != 2:\n",
    "                continue\n",
    "            if data.shape[0] > data.shape[1]:\n",
    "                data = data.T\n",
    "            if data.shape[0] != 12:\n",
    "                continue\n",
    "            if downsample is not None and downsample > 1:\n",
    "                data = data[:, ::downsample]\n",
    "            data_list.append(data)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    if not data_list:\n",
    "        raise ValueError(\"No valid data loaded for computing statistics\")\n",
    "\n",
    "    # Align lengths to stack\n",
    "    min_len = min(arr.shape[1] for arr in data_list)\n",
    "    data_list = [arr[:, :min_len] for arr in data_list]\n",
    "    stack = np.stack(data_list, axis=0)  # (N, 12, T)\n",
    "\n",
    "    mean = stack.mean(axis=(0, 2)).astype(np.float32)\n",
    "    std = (stack.std(axis=(0, 2)) + 1e-6).astype(np.float32)\n",
    "    return mean, std\n",
    "\n",
    "print(\"✓ Function defined: compute_channel_stats\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337902fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute global statistics from training set\n",
    "print(\"Computing global channel statistics from training data...\")\n",
    "global_mean, global_std = compute_channel_stats(\n",
    "    cleaned_mapping,\n",
    "    idx_train,\n",
    "    downsample=DOWNSAMPLE,\n",
    ")\n",
    "\n",
    "print(f\"\\nGlobal mean (per channel): {global_mean}\")\n",
    "print(f\"Global std (per channel):  {global_std}\")\n",
    "print(\"\\n✓ Channel statistics computed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bccc68",
   "metadata": {},
   "source": [
    "## 7. Define ECG Dataset Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60db36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for ECG data that loads from .npy files.\"\"\"\n",
    "    def __init__(self, mapping, indices, downsample=None, target_col=\"_AFIB\",\n",
    "                 channel_mean=None, channel_std=None):\n",
    "        self.mapping = mapping.iloc[indices].reset_index(drop=True)\n",
    "        self.downsample = downsample\n",
    "        self.target_col = target_col\n",
    "        self.channel_mean = channel_mean if channel_mean is not None else np.zeros(12, dtype=np.float32)\n",
    "        self.channel_std = channel_std if channel_std is not None else np.ones(12, dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mapping)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.mapping.iloc[idx]\n",
    "        record_id = str(row[\"record_id\"])\n",
    "        npy_file = Path(row[\"ecg_path\"])\n",
    "        try:\n",
    "            data = np.load(npy_file)\n",
    "            # Ensure 2D array and shape (12, L)\n",
    "            if data.ndim != 2:\n",
    "                raise ValueError(f\"Expected 2D array, got shape {data.shape}\")\n",
    "            if data.shape[0] > data.shape[1]:\n",
    "                data = data.T\n",
    "            if data.shape[0] != 12:\n",
    "                raise ValueError(f\"Expected 12 channels, got {data.shape[0]}\")\n",
    "            if self.downsample is not None and self.downsample > 1:\n",
    "                data = data[:, ::self.downsample]\n",
    "            # Normalize\n",
    "            data = (data - self.channel_mean[:, None]) / self.channel_std[:, None]\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {record_id}: {e}\")\n",
    "            # Fallback dummy tensor (kept simple)\n",
    "            data = np.zeros((12, 2500), dtype=np.float32)\n",
    "        try:\n",
    "            tval = float(row[self.target_col])\n",
    "            label = 1.0 if tval > 0 else 0.0\n",
    "        except Exception:\n",
    "            label = 0.0\n",
    "        data_tensor = torch.from_numpy(data.astype(np.float32))\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "        return data_tensor, label_tensor\n",
    "\n",
    "print(\"✓ ECGDataset class defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756cf830",
   "metadata": {},
   "source": [
    "## 8. Create DataLoaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09968898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_ds = ECGDataset(cleaned_mapping, idx_train, downsample=DOWNSAMPLE, target_col=\"_AFIB\",\n",
    "                       channel_mean=global_mean, channel_std=global_std)\n",
    "val_ds = ECGDataset(cleaned_mapping, idx_val, downsample=DOWNSAMPLE, target_col=\"_AFIB\",\n",
    "                     channel_mean=global_mean, channel_std=global_std)\n",
    "test_ds = ECGDataset(cleaned_mapping, idx_test, downsample=DOWNSAMPLE, target_col=\"_AFIB\",\n",
    "                      channel_mean=global_mean, channel_std=global_std)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Train loader: {len(train_loader)} batches\")\n",
    "print(f\"Val loader:   {len(val_loader)} batches\")\n",
    "print(f\"Test loader:  {len(test_loader)} batches\")\n",
    "print(\"\\n✓ DataLoaders created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f259028",
   "metadata": {},
   "source": [
    "## 9. Define 1D CNN Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850c0ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECG1DCNN(nn.Module):\n",
    "    def __init__(self, in_channels=12, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, 64, kernel_size=7, padding=3)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.conv4 = nn.Conv1d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(torch.relu(self.bn4(self.conv4(x))))\n",
    "        x = self.global_pool(x).squeeze(-1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = ECG1DCNN(in_channels=12, num_classes=1).to(DEVICE)\n",
    "print(f\"Model architecture:\\n{model}\")\n",
    "print(f\"\\n✓ Model initialized on {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bc50dd",
   "metadata": {},
   "source": [
    "## 10. Training Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81afc302",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"Loss function: BCEWithLogitsLoss\")\n",
    "print(f\"Optimizer: Adam (lr={LEARNING_RATE})\")\n",
    "print(\"\\n✓ Training setup complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240bd4c0",
   "metadata": {},
   "source": [
    "## 11. Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d90012",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {'train_loss': [], 'val_loss': [], 'val_auc': [], 'val_acc': []}\n",
    "best_val_auc = 0.0\n",
    "\n",
    "print(f\"Starting training for {NUM_EPOCHS} epochs...\\n\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data).squeeze()\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    avg_train_loss = train_loss / max(1, len(train_loader))\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data).squeeze()\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item()\n",
    "            probs = torch.sigmoid(output).cpu().numpy()\n",
    "            all_preds.extend(probs)\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "    avg_val_loss = val_loss / max(1, len(val_loader))\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "    val_auc = roc_auc_score(all_targets, all_preds) if len(np.unique(all_targets)) > 1 else 0.5\n",
    "    val_acc = accuracy_score(all_targets, (all_preds > 0.5).astype(int))\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    history['val_loss'].append(avg_val_loss)\n",
    "    history['val_auc'].append(val_auc)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val AUC: {val_auc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "        print(f\"  → Best model saved (AUC: {val_auc:.4f})\")\n",
    "\n",
    "print(f\"\\n✓ Training complete!\")\n",
    "print(f\"Best validation AUC: {best_val_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a560cf22",
   "metadata": {},
   "source": [
    "## 12. Plot Training History\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340c4fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "axes[0].plot(history['train_loss'], label='Train Loss')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training & Validation Loss')\n",
    "axes[0].legend(); axes[0].grid(True)\n",
    "axes[1].plot(history['val_auc'], label='Val AUC', color='green')\n",
    "axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('AUC'); axes[1].set_title('Validation AUC')\n",
    "axes[1].legend(); axes[1].grid(True)\n",
    "axes[2].plot(history['val_acc'], label='Val Accuracy', color='orange')\n",
    "axes[2].set_xlabel('Epoch'); axes[2].set_ylabel('Accuracy'); axes[2].set_title('Validation Accuracy')\n",
    "axes[2].legend(); axes[2].grid(True)\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96861a5a",
   "metadata": {},
   "source": [
    "## 13. Load Best Model & Evaluate on Test Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0d2b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded best model from {MODEL_PATH}\")\n",
    "print(\"\\nEvaluating on test set...\\n\")\n",
    "\n",
    "test_loss = 0.0\n",
    "all_test_preds, all_test_targets = [], []\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        output = model(data).squeeze()\n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss.item()\n",
    "        probs = torch.sigmoid(output).cpu().numpy()\n",
    "        all_test_preds.extend(probs)\n",
    "        all_test_targets.extend(target.cpu().numpy())\n",
    "avg_test_loss = test_loss / max(1, len(test_loader))\n",
    "all_test_preds = np.array(all_test_preds)\n",
    "all_test_targets = np.array(all_test_targets)\n",
    "\n",
    "test_auc = roc_auc_score(all_test_targets, all_test_preds) if len(np.unique(all_test_targets)) > 1 else 0.5\n",
    "test_pred_labels = (all_test_preds > 0.5).astype(int)\n",
    "test_acc = accuracy_score(all_test_targets, test_pred_labels)\n",
    "conf_matrix = confusion_matrix(all_test_targets, test_pred_labels)\n",
    "\n",
    "print(\"--- Test Set Results ---\")\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "print(f\"Test AUC:  {test_auc:.4f}\")\n",
    "print(f\"Test Acc:  {test_acc:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_test_targets, test_pred_labels, target_names=['Non-AFIB', 'AFIB']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a2eebf",
   "metadata": {},
   "source": [
    "## 14. Visualize Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c54510",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Non-AFIB', 'AFIB'],\n",
    "            yticklabels=['Non-AFIB', 'AFIB'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title(f'Confusion Matrix (Test Set)\\nAUC: {test_auc:.4f}, Acc: {test_acc:.4f}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n============================================================\")\n",
    "print(\"✓ AFIB detection pipeline complete!\")\n",
    "print(\"============================================================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
