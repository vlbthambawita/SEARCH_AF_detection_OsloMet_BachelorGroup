{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c7e755b",
   "metadata": {},
   "source": [
    "# AFIB Detection Using 1D U-Net on Cleaned ECG Dataset\n",
    "\n",
    "This notebook mirrors the data handling of the 1D-CNN pipeline but uses a 1D U-Net-style model for binary AFIB detection (AFIB vs SR).\n",
    "\n",
    "Workflow:\n",
    "1. Setup paths and load cleaned mapping\n",
    "2. Verify data and prepare labels\n",
    "3. Split dataset (train/val/test)\n",
    "4. Compute global channel statistics\n",
    "5. Define ECG dataset class\n",
    "6. Create data loaders\n",
    "7. Define and train 1D U-Net model\n",
    "8. Evaluate on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8af190",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8aa0991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, precision_score, recall_score,\n",
    "    f1_score, matthews_corrcoef, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ae57ef",
   "metadata": {},
   "source": [
    "## 2. Configuration & Path Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5cd831d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "PROJECT_ROOT: c:\\Users\\Adrian\\VSCode_Projects\\bachelorPrep\\SEARCH_AF_detection_OsloMet_BachelorGroup\\src\n",
      "DATA_DIR: c:\\Users\\Adrian\\VSCode_Projects\\bachelorPrep\\SEARCH_AF_detection_OsloMet_BachelorGroup\\src\\data\n",
      "MAPPING_CSV: c:\\Users\\Adrian\\VSCode_Projects\\bachelorPrep\\SEARCH_AF_detection_OsloMet_BachelorGroup\\src\\data\\cleaned_balanced_AFIB_SR\\file_mapping_cleaned.csv\n",
      "MODEL_PATH: c:\\Users\\Adrian\\VSCode_Projects\\bachelorPrep\\SEARCH_AF_detection_OsloMet_BachelorGroup\\src\\models\\ecg_1dunet_afib_balanced.pth\n",
      "\n",
      "✓ Configuration complete\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "DOWNSAMPLE = 2  # Downsample factor (e.g., 2 = keep every 2nd sample)\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 30\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paths (all relative to project root where notebook is located)\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "CLEANED_ROOT = DATA_DIR / \"cleaned_balanced_AFIB_SR\"\n",
    "CLEANED_WFDB_DIR = CLEANED_ROOT / \"WFDBRecords\"\n",
    "MAPPING_CSV = CLEANED_ROOT / \"file_mapping_cleaned.csv\"\n",
    "\n",
    "# Model save path\n",
    "MODEL_DIR = PROJECT_ROOT / \"models\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_PATH = MODEL_DIR / \"ecg_1dunet_afib_balanced.pth\"\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"PROJECT_ROOT: {PROJECT_ROOT}\")\n",
    "print(f\"DATA_DIR: {DATA_DIR}\")\n",
    "print(f\"MAPPING_CSV: {MAPPING_CSV}\")\n",
    "print(f\"MODEL_PATH: {MODEL_PATH}\")\n",
    "print(\"\\n✓ Configuration complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cc2699",
   "metadata": {},
   "source": [
    "## 3. Load Cleaned Mapping & Verify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fefeceb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3,555 records from file_mapping_cleaned.csv\n",
      "\n",
      "Columns: ['record_id', 'ecg_path', '_AFIB', '_SR']\n",
      "✓ All required columns present: ['record_id', 'ecg_path', '_AFIB', '_SR']\n",
      "\n",
      "--- Dataset Statistics ---\n",
      "Total records: 3,555\n",
      "AFIB records (_AFIB=1): 1,780\n",
      "Non-AFIB records (_AFIB=0): 1,775\n",
      "SR records (_SR=1): 1,775\n",
      "Non-SR records (_SR=0): 1,780\n",
      "\n",
      "Sample rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_id</th>\n",
       "      <th>ecg_path</th>\n",
       "      <th>_AFIB</th>\n",
       "      <th>_SR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JS36340</td>\n",
       "      <td>c:\\Users\\Adrian\\VSCode_Projects\\bachelorPrep\\S...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JS04305</td>\n",
       "      <td>c:\\Users\\Adrian\\VSCode_Projects\\bachelorPrep\\S...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JS11050</td>\n",
       "      <td>c:\\Users\\Adrian\\VSCode_Projects\\bachelorPrep\\S...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JS23632</td>\n",
       "      <td>c:\\Users\\Adrian\\VSCode_Projects\\bachelorPrep\\S...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JS41351</td>\n",
       "      <td>c:\\Users\\Adrian\\VSCode_Projects\\bachelorPrep\\S...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  record_id                                           ecg_path  _AFIB  _SR\n",
       "0   JS36340  c:\\Users\\Adrian\\VSCode_Projects\\bachelorPrep\\S...      0    1\n",
       "1   JS04305  c:\\Users\\Adrian\\VSCode_Projects\\bachelorPrep\\S...      1    0\n",
       "2   JS11050  c:\\Users\\Adrian\\VSCode_Projects\\bachelorPrep\\S...      0    1\n",
       "3   JS23632  c:\\Users\\Adrian\\VSCode_Projects\\bachelorPrep\\S...      0    1\n",
       "4   JS41351  c:\\Users\\Adrian\\VSCode_Projects\\bachelorPrep\\S...      0    1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the cleaned mapping\n",
    "cleaned_mapping = pd.read_csv(MAPPING_CSV)\n",
    "\n",
    "print(f\"Loaded {len(cleaned_mapping):,} records from {MAPPING_CSV.name}\")\n",
    "print(f\"\\nColumns: {list(cleaned_mapping.columns)}\")\n",
    "\n",
    "# Verify required columns exist\n",
    "required_cols = ['record_id', 'ecg_path', '_AFIB', '_SR']\n",
    "missing_cols = [col for col in required_cols if col not in cleaned_mapping.columns]\n",
    "assert not missing_cols, f\"Missing required columns: {missing_cols}\"\n",
    "print(f\"✓ All required columns present: {required_cols}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\n--- Dataset Statistics ---\")\n",
    "print(f\"Total records: {len(cleaned_mapping):,}\")\n",
    "print(f\"AFIB records (_AFIB=1): {cleaned_mapping['_AFIB'].sum():,}\")\n",
    "print(f\"Non-AFIB records (_AFIB=0): {(cleaned_mapping['_AFIB'] == 0).sum():,}\")\n",
    "print(f\"SR records (_SR=1): {cleaned_mapping['_SR'].sum():,}\")\n",
    "print(f\"Non-SR records (_SR=0): {(cleaned_mapping['_SR'] == 0).sum():,}\")\n",
    "\n",
    "# Display sample rows\n",
    "print(\"\\nSample rows:\")\n",
    "cleaned_mapping.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886acf7f",
   "metadata": {},
   "source": [
    "## 4. Prepare Labels for Binary AFIB Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b575098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target column: _AFIB\n",
      "Target distribution:\n",
      "_AFIB\n",
      "0    1775\n",
      "1    1780\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✓ Labels prepared for binary AFIB detection\n"
     ]
    }
   ],
   "source": [
    "# Use _AFIB as the primary target column\n",
    "target_col = \"_AFIB\"\n",
    "\n",
    "# Ensure target is strictly 0/1\n",
    "cleaned_mapping[target_col] = pd.to_numeric(cleaned_mapping[target_col], errors='coerce').fillna(0)\n",
    "cleaned_mapping[target_col] = (cleaned_mapping[target_col] > 0).astype(int)\n",
    "\n",
    "print(f\"Target column: {target_col}\")\n",
    "print(\"Target distribution:\")\n",
    "print(cleaned_mapping[target_col].value_counts().sort_index())\n",
    "print(\"\\n✓ Labels prepared for binary AFIB detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110f1b74",
   "metadata": {},
   "source": [
    "## 5. Split Dataset (Train/Val/Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a675583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Dataset Splits ---\n",
      "Train: 2,844 samples (AFIB: 1,424, Non-AFIB: 1,420)\n",
      "Val:   355 samples (AFIB: 178, Non-AFIB: 177)\n",
      "Test:  356 samples (AFIB: 178, Non-AFIB: 178)\n",
      "\n",
      "✓ Dataset split complete\n"
     ]
    }
   ],
   "source": [
    "# Create indices array\n",
    "indices = np.arange(len(cleaned_mapping))\n",
    "labels = cleaned_mapping[target_col].values\n",
    "\n",
    "# First split: 80% train, 20% temp\n",
    "idx_train, idx_temp, y_train, y_temp = train_test_split(\n",
    "    indices, labels,\n",
    "    test_size=0.2,\n",
    "    stratify=labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Second split: temp into 50% val, 50% test\n",
    "idx_val, idx_test, y_val, y_test = train_test_split(\n",
    "    idx_temp, y_temp,\n",
    "    test_size=0.5,\n",
    "    stratify=y_temp,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"--- Dataset Splits ---\")\n",
    "print(f\"Train: {len(idx_train):,} samples (AFIB: {y_train.sum():,}, Non-AFIB: {(y_train == 0).sum():,})\")\n",
    "print(f\"Val:   {len(idx_val):,} samples (AFIB: {y_val.sum():,}, Non-AFIB: {(y_val == 0).sum():,})\")\n",
    "print(f\"Test:  {len(idx_test):,} samples (AFIB: {y_test.sum():,}, Non-AFIB: {(y_test == 0).sum():,})\")\n",
    "print(\"\\n✓ Dataset split complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129255c1",
   "metadata": {},
   "source": [
    "## 6. Compute Global Channel Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66abf1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Function defined: compute_channel_stats\n"
     ]
    }
   ],
   "source": [
    "def compute_channel_stats(mapping, indices, downsample=None, max_samples=512):\n",
    "    \"\"\"\n",
    "    Compute global per-channel mean and std from a subset of training data.\n",
    "    Stacks up to max_samples random samples to estimate statistics.\n",
    "    \"\"\"\n",
    "    if len(indices) > max_samples:\n",
    "        sample_indices = np.random.choice(indices, size=max_samples, replace=False)\n",
    "    else:\n",
    "        sample_indices = indices\n",
    "\n",
    "    data_list = []\n",
    "    checked_count = 0\n",
    "    loaded_count = 0\n",
    "\n",
    "    for idx in sample_indices:\n",
    "        row = mapping.iloc[int(idx)]\n",
    "        npy_file = Path(row[\"ecg_path\"])\n",
    "        \n",
    "        # Debug: print first few paths\n",
    "        if checked_count < 3:\n",
    "            print(f\"  Checking path {checked_count + 1}: {npy_file}\")\n",
    "        \n",
    "        # Resolve relative paths against PROJECT_ROOT\n",
    "        if not npy_file.is_absolute():\n",
    "            npy_file = (PROJECT_ROOT / npy_file).resolve()\n",
    "            if checked_count < 3:\n",
    "                print(f\"    Resolved to: {npy_file}\")\n",
    "        \n",
    "        checked_count += 1\n",
    "        \n",
    "        if not npy_file.exists():\n",
    "            if checked_count <= 3:\n",
    "                print(f\"    File does not exist!\")\n",
    "            continue\n",
    "        \n",
    "        if checked_count <= 3:\n",
    "            print(f\"    File exists! Loading...\")\n",
    "            \n",
    "        try:\n",
    "            data = np.load(npy_file)\n",
    "            # Ensure 2D array and shape (12, L) or (L, 12)\n",
    "            if data.ndim != 2:\n",
    "                continue\n",
    "            if data.shape[0] > data.shape[1]:\n",
    "                # Possibly (L, 12); transpose to (12, L)\n",
    "                data = data.T\n",
    "            if data.shape[0] != 12:\n",
    "                continue\n",
    "            if downsample is not None and downsample > 1:\n",
    "                data = data[:, ::downsample]\n",
    "            # Replace NaN/Inf\n",
    "            data = np.nan_to_num(data, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            data_list.append(data)\n",
    "            loaded_count += 1\n",
    "        except Exception as e:\n",
    "            if checked_count <= 3:\n",
    "                print(f\"    Error loading: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"  Checked {checked_count} files, successfully loaded {loaded_count}\")\n",
    "    \n",
    "    if not data_list:\n",
    "        raise ValueError(f\"No valid data loaded for computing statistics (checked {checked_count} paths)\")\n",
    "\n",
    "    # Align lengths to stack\n",
    "    min_len = min(arr.shape[1] for arr in data_list)\n",
    "    data_list = [arr[:, :min_len] for arr in data_list]\n",
    "    stack = np.stack(data_list, axis=0)  # (N, 12, T)\n",
    "\n",
    "    mean = stack.mean(axis=(0, 2)).astype(np.float32)\n",
    "    std = (stack.std(axis=(0, 2)) + 1e-6).astype(np.float32)\n",
    "    mean = np.nan_to_num(mean, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    std = np.nan_to_num(std, nan=1.0, posinf=1.0, neginf=1.0)\n",
    "    return mean, std\n",
    "\n",
    "print(\"✓ Function defined: compute_channel_stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "965d2f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing global channel statistics from training data...\n",
      "  Checking path 1: c:\\Users\\Adrian\\VSCode_Projects\\bachelorPrep\\SEARCH_AF_detection_OsloMet_BachelorGroup\\data\\cleaned_balanced_AFIB_SR\\WFDBRecords\\JS\\JS03437\\JS03437.npy\n",
      "    File exists! Loading...\n",
      "  Checking path 2: c:\\Users\\Adrian\\VSCode_Projects\\bachelorPrep\\SEARCH_AF_detection_OsloMet_BachelorGroup\\data\\cleaned_balanced_AFIB_SR\\WFDBRecords\\JS\\JS40338\\JS40338.npy\n",
      "    File exists! Loading...\n",
      "  Checking path 3: c:\\Users\\Adrian\\VSCode_Projects\\bachelorPrep\\SEARCH_AF_detection_OsloMet_BachelorGroup\\data\\cleaned_balanced_AFIB_SR\\WFDBRecords\\JS\\JS00215\\JS00215.npy\n",
      "    File exists! Loading...\n",
      "  Checked 512 files, successfully loaded 512\n",
      "\n",
      "Global mean (per channel): [-0.00011662  0.00371705  0.00383266 -0.00060558 -0.00316764  0.00258137\n",
      " -0.00272983  0.00090673  0.00061893  0.00311584 -0.00016864 -0.00210504]\n",
      "Global std (per channel):  [0.13181873 0.15539527 0.12943582 0.12874106 0.10502099 0.12691747\n",
      " 0.19367273 0.3277205  0.33422276 0.34579355 0.32107964 0.30704993]\n",
      "\n",
      "✓ Channel statistics computed\n"
     ]
    }
   ],
   "source": [
    "# Compute global statistics from training set\n",
    "print(\"Computing global channel statistics from training data...\")\n",
    "global_mean, global_std = compute_channel_stats(\n",
    "    cleaned_mapping,\n",
    "    idx_train,\n",
    "    downsample=DOWNSAMPLE,\n",
    "    max_samples=512\n",
    ")\n",
    "\n",
    "print(f\"\\nGlobal mean (per channel): {global_mean}\")\n",
    "print(f\"Global std (per channel):  {global_std}\")\n",
    "print(\"\\n✓ Channel statistics computed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422d8971",
   "metadata": {},
   "source": [
    "## 7. Define ECG Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c9b8164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ECGDataset class defined\n"
     ]
    }
   ],
   "source": [
    "class ECGDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for ECG data that loads from .npy files.\"\"\"\n",
    "    def __init__(self, mapping, indices, downsample=None, target_col=\"_AFIB\",\n",
    "                 channel_mean=None, channel_std=None):\n",
    "        self.mapping = mapping.iloc[indices].reset_index(drop=True)\n",
    "        self.downsample = downsample\n",
    "        self.target_col = target_col\n",
    "        self.channel_mean = channel_mean if channel_mean is not None else np.zeros(12, dtype=np.float32)\n",
    "        self.channel_std = channel_std if channel_std is not None else np.ones(12, dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mapping)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.mapping.iloc[idx]\n",
    "        record_id = str(row[\"record_id\"])\n",
    "        npy_file = Path(row[\"ecg_path\"])\n",
    "        # Resolve relative paths against PROJECT_ROOT\n",
    "        if not npy_file.is_absolute():\n",
    "            npy_file = (PROJECT_ROOT / npy_file).resolve()\n",
    "        if not npy_file.exists():\n",
    "            # Fallback dummy\n",
    "            data = np.zeros((12, 2500), dtype=np.float32)\n",
    "        else:\n",
    "            try:\n",
    "                data = np.load(npy_file)\n",
    "                # Ensure 2D array and shape (12, L)\n",
    "                if data.ndim != 2:\n",
    "                    raise ValueError(f\"Expected 2D array, got shape {data.shape}\")\n",
    "                if data.shape[0] > data.shape[1]:\n",
    "                    data = data.T\n",
    "                if data.shape[0] != 12:\n",
    "                    raise ValueError(f\"Expected 12 channels, got {data.shape[0]}\")\n",
    "                if self.downsample is not None and self.downsample > 1:\n",
    "                    data = data[:, ::self.downsample]\n",
    "                # Replace NaN/Inf\n",
    "                data = np.nan_to_num(data, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                # Normalize\n",
    "                data = (data - self.channel_mean[:, None]) / self.channel_std[:, None]\n",
    "                data = np.nan_to_num(data, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                data = data.astype(np.float32)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {record_id}: {e}\")\n",
    "                data = np.zeros((12, 2500), dtype=np.float32)\n",
    "        try:\n",
    "            tval = float(row[self.target_col])\n",
    "            label = 1.0 if tval > 0 else 0.0\n",
    "        except Exception:\n",
    "            label = 0.0\n",
    "        data_tensor = torch.from_numpy(data).float()\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "        return data_tensor, label_tensor\n",
    "\n",
    "print(\"✓ ECGDataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ed2bfd",
   "metadata": {},
   "source": [
    "## 8. Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02eb5059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader: 89 batches\n",
      "Val loader:   12 batches\n",
      "Test loader:  12 batches\n",
      "\n",
      "✓ DataLoaders created\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_ds = ECGDataset(cleaned_mapping, idx_train, downsample=DOWNSAMPLE, target_col=\"_AFIB\",\n",
    "                       channel_mean=global_mean, channel_std=global_std)\n",
    "val_ds = ECGDataset(cleaned_mapping, idx_val, downsample=DOWNSAMPLE, target_col=\"_AFIB\",\n",
    "                     channel_mean=global_mean, channel_std=global_std)\n",
    "test_ds = ECGDataset(cleaned_mapping, idx_test, downsample=DOWNSAMPLE, target_col=\"_AFIB\",\n",
    "                      channel_mean=global_mean, channel_std=global_std)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Train loader: {len(train_loader)} batches\")\n",
    "print(f\"Val loader:   {len(val_loader)} batches\")\n",
    "print(f\"Test loader:  {len(test_loader)} batches\")\n",
    "print(\"\\n✓ DataLoaders created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3f765a",
   "metadata": {},
   "source": [
    "## 9. Define 1D U-Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1103a868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture:\n",
      "UNet1D(\n",
      "  (enc1): Sequential(\n",
      "    (0): Conv1d(12, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (enc2): Sequential(\n",
      "    (0): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (enc3): Sequential(\n",
      "    (0): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (pool3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (bottleneck): Sequential(\n",
      "    (0): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (up3): ConvTranspose1d(256, 128, kernel_size=(2,), stride=(2,))\n",
      "  (dec3): Sequential(\n",
      "    (0): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (up2): ConvTranspose1d(128, 64, kernel_size=(2,), stride=(2,))\n",
      "  (dec2): Sequential(\n",
      "    (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (up1): ConvTranspose1d(64, 32, kernel_size=(2,), stride=(2,))\n",
      "  (dec1): Sequential(\n",
      "    (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (out): Conv1d(32, 1, kernel_size=(1,), stride=(1,))\n",
      "  (global_pool): AdaptiveAvgPool1d(output_size=1)\n",
      ")\n",
      "\n",
      "✓ 1D U-Net model initialized on cpu\n"
     ]
    }
   ],
   "source": [
    "def conv_block(in_ch, out_ch, k=3, p=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv1d(in_ch, out_ch, kernel_size=k, padding=p),\n",
    "        nn.BatchNorm1d(out_ch),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv1d(out_ch, out_ch, kernel_size=k, padding=p),\n",
    "        nn.BatchNorm1d(out_ch),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "class UNet1D(nn.Module):\n",
    "    def __init__(self, in_channels=12, base_ch=32):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = conv_block(in_channels, base_ch)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.enc2 = conv_block(base_ch, base_ch*2)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        self.enc3 = conv_block(base_ch*2, base_ch*4)\n",
    "        self.pool3 = nn.MaxPool1d(2)\n",
    "        self.bottleneck = conv_block(base_ch*4, base_ch*8)\n",
    "        # Decoder\n",
    "        self.up3 = nn.ConvTranspose1d(base_ch*8, base_ch*4, kernel_size=2, stride=2)\n",
    "        self.dec3 = conv_block(base_ch*8, base_ch*4)\n",
    "        self.up2 = nn.ConvTranspose1d(base_ch*4, base_ch*2, kernel_size=2, stride=2)\n",
    "        self.dec2 = conv_block(base_ch*4, base_ch*2)\n",
    "        self.up1 = nn.ConvTranspose1d(base_ch*2, base_ch, kernel_size=2, stride=2)\n",
    "        self.dec1 = conv_block(base_ch*2, base_ch)\n",
    "        # Output head: per-time-step logit\n",
    "        self.out = nn.Conv1d(base_ch, 1, kernel_size=1)\n",
    "        # Global pooling to get single logit per record\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)\n",
    "        p1 = self.pool1(e1)\n",
    "        e2 = self.enc2(p1)\n",
    "        p2 = self.pool2(e2)\n",
    "        e3 = self.enc3(p2)\n",
    "        p3 = self.pool3(e3)\n",
    "        b = self.bottleneck(p3)\n",
    "        # Decoder with dimension alignment at skip connections\n",
    "        u3 = self.up3(b)\n",
    "        min_len = min(u3.shape[-1], e3.shape[-1])\n",
    "        u3 = u3[..., :min_len]\n",
    "        e3_crop = e3[..., :min_len]\n",
    "        d3 = self.dec3(torch.cat([u3, e3_crop], dim=1))\n",
    "        \n",
    "        u2 = self.up2(d3)\n",
    "        min_len = min(u2.shape[-1], e2.shape[-1])\n",
    "        u2 = u2[..., :min_len]\n",
    "        e2_crop = e2[..., :min_len]\n",
    "        d2 = self.dec2(torch.cat([u2, e2_crop], dim=1))\n",
    "        \n",
    "        u1 = self.up1(d2)\n",
    "        min_len = min(u1.shape[-1], e1.shape[-1])\n",
    "        u1 = u1[..., :min_len]\n",
    "        e1_crop = e1[..., :min_len]\n",
    "        d1 = self.dec1(torch.cat([u1, e1_crop], dim=1))\n",
    "        \n",
    "        # Time logits and global pooling\n",
    "        tlogits = self.out(d1)  # (B, 1, T)\n",
    "        pooled = self.global_pool(tlogits).squeeze(-1)  # (B, 1)\n",
    "        return pooled.squeeze(1)  # (B)\n",
    "\n",
    "model = UNet1D(in_channels=12, base_ch=32).to(DEVICE)\n",
    "print(f\"Model architecture:\\n{model}\")\n",
    "print(f\"\\n✓ 1D U-Net model initialized on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031365ce",
   "metadata": {},
   "source": [
    "## 10. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9aba5331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function: BCEWithLogitsLoss\n",
      "Optimizer: Adam (lr=0.001)\n",
      "\n",
      "✓ Training setup complete\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"Loss function: BCEWithLogitsLoss\")\n",
    "print(f\"Optimizer: Adam (lr={LEARNING_RATE})\")\n",
    "print(\"\\n✓ Training setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d85416",
   "metadata": {},
   "source": [
    "## 11. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cec684fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 30 epochs...\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 624 but got size 625 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m data, target = data.to(DEVICE), target.to(DEVICE)\n\u001b[32m     10\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B)\u001b[39;00m\n\u001b[32m     12\u001b[39m loss = criterion(output, target)\n\u001b[32m     13\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Adrian\\VSCode_Projects\\bachelorPrep\\SEARCH_AF_detection_OsloMet_BachelorGroup\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Adrian\\VSCode_Projects\\bachelorPrep\\SEARCH_AF_detection_OsloMet_BachelorGroup\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mUNet1D.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Decoder\u001b[39;00m\n\u001b[32m     44\u001b[39m u3 = \u001b[38;5;28mself\u001b[39m.up3(b)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m d3 = \u001b[38;5;28mself\u001b[39m.dec3(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mu3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me3\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m     46\u001b[39m u2 = \u001b[38;5;28mself\u001b[39m.up2(d3)\n\u001b[32m     47\u001b[39m d2 = \u001b[38;5;28mself\u001b[39m.dec2(torch.cat([u2, e2], dim=\u001b[32m1\u001b[39m))\n",
      "\u001b[31mRuntimeError\u001b[39m: Sizes of tensors must match except in dimension 1. Expected size 624 but got size 625 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "history = {'train_loss': [], 'val_loss': [], 'val_auc': [], 'val_acc': [], 'val_f1': []}\n",
    "best_val_auc = 0.0\n",
    "\n",
    "print(f\"Starting training for {NUM_EPOCHS} epochs...\\n\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)  # (B)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    avg_train_loss = train_loss / max(1, len(train_loader))\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)  # (B)\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item()\n",
    "            probs = torch.sigmoid(output).cpu().numpy()\n",
    "            all_preds.extend(probs)\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "    avg_val_loss = val_loss / max(1, len(val_loader))\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "    val_auc = roc_auc_score(all_targets, all_preds) if len(np.unique(all_targets)) > 1 else 0.5\n",
    "    val_acc = accuracy_score(all_targets, (all_preds > 0.5).astype(int))\n",
    "    val_f1 = f1_score(all_targets, (all_preds > 0.5).astype(int), zero_division=0)\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    history['val_loss'].append(avg_val_loss)\n",
    "    history['val_auc'].append(val_auc)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_f1'].append(val_f1)\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val AUC: {val_auc:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "        print(f\"  → Best model saved (AUC: {val_auc:.4f})\")\n",
    "\n",
    "print(f\"\\n✓ Training complete!\")\n",
    "print(f\"Best validation AUC: {best_val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5c0ecb",
   "metadata": {},
   "source": [
    "## 12. Load Best Model & Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d2dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded best model from {MODEL_PATH}\")\n",
    "print(\"\\nEvaluating on test set...\\n\")\n",
    "\n",
    "test_loss = 0.0\n",
    "all_test_preds, all_test_targets = [], []\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        output = model(data)  # (B)\n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss.item()\n",
    "        probs = torch.sigmoid(output).cpu().numpy()\n",
    "        all_test_preds.extend(probs)\n",
    "        all_test_targets.extend(target.cpu().numpy())\n",
    "avg_test_loss = test_loss / max(1, len(test_loader))\n",
    "all_test_preds = np.array(all_test_preds)\n",
    "all_test_targets = np.array(all_test_targets)\n",
    "\n",
    "test_auc = roc_auc_score(all_test_targets, all_test_preds) if len(np.unique(all_test_targets)) > 1 else 0.5\n",
    "test_pred_labels = (all_test_preds > 0.5).astype(int)\n",
    "test_acc = accuracy_score(all_test_targets, test_pred_labels)\n",
    "test_pre = precision_score(all_test_targets, test_pred_labels, zero_division=0)\n",
    "test_rec = recall_score(all_test_targets, test_pred_labels, zero_division=0)\n",
    "cm = confusion_matrix(all_test_targets, test_pred_labels, labels=[0,1])\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "test_spec = TN / (TN + FP) if (TN + FP) > 0 else np.nan\n",
    "test_f1 = f1_score(all_test_targets, test_pred_labels, zero_division=0)\n",
    "test_mcc = matthews_corrcoef(all_test_targets, test_pred_labels)\n",
    "\n",
    "print(\"--- Test Set Results ---\")\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "print(f\"Test AUC:  {test_auc:.4f}\")\n",
    "print(f\"Test Acc:  {test_acc:.4f}\")\n",
    "print(f\"Test Pre:  {test_pre:.4f}\")\n",
    "print(f\"Test Rec:  {test_rec:.4f}\")\n",
    "print(f\"Test Spec: {test_spec:.4f}\")\n",
    "print(f\"Test F1:   {test_f1:.4f}\")\n",
    "print(f\"Test MCC:  {test_mcc:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_test_targets, test_pred_labels, target_names=['Non-AFIB', 'AFIB']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443d06a9",
   "metadata": {},
   "source": [
    "## 13. Visualize Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11e900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Non-AFIB', 'AFIB'],\n",
    "            yticklabels=['Non-AFIB', 'AFIB'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title(f'Confusion Matrix (Test Set)\\nAUC: {test_auc:.4f}, Acc: {test_acc:.4f}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n============================================================\")\n",
    "print(\"✓ AFIB detection pipeline complete!\")\n",
    "print(\"============================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd34ae2e",
   "metadata": {},
   "source": [
    "## 14. ROC Curve (Receiver Operating Characteristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce6431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(all_test_targets, all_test_preds)\n",
    "roc_auc_curve = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc_curve:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ROC-AUC Score: {roc_auc_curve:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78a4b4c",
   "metadata": {},
   "source": [
    "## 15. DET Curve (Detection Error Tradeoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1d30fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import DetCurveDisplay\n",
    "\n",
    "# Create and plot DET curve\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "DetCurveDisplay.from_predictions(all_test_targets, all_test_preds, ax=ax, name='AFIB Detection')\n",
    "\n",
    "ax.set_title('Detection Error Tradeoff (DET) Curve', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('False Negative Rate', fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(loc='upper right', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"DET Curve: Lower values indicate better performance\")\n",
    "print(f\"The curve shows the tradeoff between false positives and false negatives\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee48708",
   "metadata": {},
   "source": [
    "## 16. Metrics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a01656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute all metrics from test results\n",
    "acc = test_acc\n",
    "f1 = test_f1\n",
    "pre = test_pre\n",
    "rec = test_rec\n",
    "spec = test_spec\n",
    "mcc = test_mcc\n",
    "roc_auc = test_auc\n",
    "\n",
    "print(f\"Acc:  {acc:.3f}\")\n",
    "print(f\"F1:   {f1:.3f}\")\n",
    "print(f\"Pre:  {pre:.3f}\")\n",
    "print(f\"Rec:  {rec:.3f}\")\n",
    "print(f\"Spec: {spec:.3f}\")\n",
    "print(f\"MCC:  {mcc:.3f}\")\n",
    "print(f\"AUC:  {roc_auc:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
